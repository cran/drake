<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>The concept</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>



<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<p><code>Drake</code> has extensive high-performance computing support, from local multicore parallelism to serious distributed computing across multiple nodes of a cluster. Control it with the <code>parallelism</code> and <code>jobs</code> arguments to <code>make()</code>, and use <code>future::plan()</code> if <code>parallelism</code> is <code>&quot;future_lapply&quot;</code>.</p>

<h1>The concept</h1>

<p><code>Drake</code>&#39;s approach to parallelism relies on the network graph representation of a project.</p>

<pre><code class="r">clean()
load_basic_example()
config &lt;- make(my_plan, jobs = 2, verbose = FALSE) # Parallelize over 2 jobs.
# Change a dependency.
reg2 &lt;- function(d) {
  d$x3 &lt;- d$x ^ 3
  lm(y ~ x3, data = d)
}
# Hover, click, drag, zoom, and pan.
vis_drake_graph(config, width = &quot;100%&quot;, height = &quot;500px&quot;)
</code></pre>

<p><iframe
src = "https://ropensci.github.io/drake/images/reg2.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe></p>

<p>The nodes in each column above are conditionally independent given the dependencies to the left. So in general, the targets and imports are processed column by column from left to right, and everything within a column is executed in parallel. When some targets are already up to date, <code>drake</code> searches ahead in the graph to maximize the number of outdated targets in each parallelizable stage.</p>

<p>To show the parallelizable stages of the next <code>make()</code> programmatically, use the <code>parallel_stages()</code> function. All the targets/imports in a stage are processed in parallel before moving on to the next stage.</p>

<pre><code class="r">parallel_stages(config)
##                      item imported  file stage
## 1            &quot;report.Rmd&quot;     TRUE  TRUE     1
## 2              data.frame     TRUE FALSE     1
## 3                    knit     TRUE FALSE     1
## 4                      lm     TRUE FALSE     1
## 5                  mtcars     TRUE FALSE     1
## 6                    nrow     TRUE FALSE     1
## 7              sample.int     TRUE FALSE     1
## 8                 summary     TRUE FALSE     1
## 9        suppressWarnings     TRUE FALSE     1
## 10            random_rows     TRUE FALSE     2
## 11                   reg1     TRUE FALSE     2
## 12                   reg2     TRUE FALSE     2
## 13               simulate     TRUE FALSE     3
## 14      regression2_large    FALSE FALSE     4
## 15      regression2_small    FALSE FALSE     4
## 16 coef_regression2_large    FALSE FALSE     5
## 17 coef_regression2_small    FALSE FALSE     5
## 18 summ_regression2_large    FALSE FALSE     5
## 19 summ_regression2_small    FALSE FALSE     5
## 20            &quot;report.md&quot;    FALSE  TRUE     6
</code></pre>

<h1>How many parallel jobs should you use?</h1>

<h2>Not too many!</h2>

<p>Be mindful of the maximum number of simultaneous parallel jobs you deploy. Consequences of greed and carelessness range from poor etiquette to system crashes. In most cases, the <code>jobs</code> argument to <code>make()</code> sets the maximum number of simultaneous jobs, but it does not apply to the parallel execution of targets when <code>parallelism</code> is <code>&quot;future_lapply&quot;</code>. If you use <code>&quot;future_lapply&quot;</code> parallelism, please see the  <code>workers</code> argument to most supporting functions passed to <code>future::plan()</code> (for example, <code>future::plan(multisession(workers = 2))</code>). Depending on the <a href="https://github.com/HenrikBengtsson/future">future</a> backend you select with <code>future::plan()</code> or <code>future::plan()</code>, you might also make use of one of the other environment variables listed in <code>?future::future.options</code>.</p>

<h2>Drake can report the maximum number of useful simultaneous jobs</h2>

<p>The <code>max_useful_jobs()</code> function analyzes your project and recommends a maximum value for the <code>jobs</code> argument to the next <code>make()</code> (or the <code>workers</code> argument to a backend function in <a href="https://github.com/HenrikBengtsson/future">future</a>). This number returned by <code>max_useful_jobs()</code> is only an upper bound, not necessarily the number of <code>jobs</code> you should choose. </p>

<pre><code class="r">library(drake)
load_basic_example()
config &lt;- drake_config(my_plan)
vis_drake_graph(config) # Set targets_only to TRUE for smaller graphs.
max_useful_jobs(config) # 8
max_useful_jobs(config, imports = &quot;files&quot;) # 8
max_useful_jobs(config, imports = &quot;all&quot;) # 8
max_useful_jobs(config, imports = &quot;none&quot;) # 8
config &lt;- make(my_plan, jobs = 4)
vis_drake_graph(config)
# Ignore the targets already built.
max_useful_jobs(config) # 1
max_useful_jobs(config, imports = &quot;files&quot;) # 1
max_useful_jobs(config, imports = &quot;all&quot;) # 8
max_useful_jobs(config, imports = &quot;none&quot;) # 0
# Change a function so some targets are now out of date.
reg2 &lt;- function(d){
  d$x3 &lt;- d$x ^ 3
  lm(y ~ x3, data = d)
}
vis_drake_graph(config)
max_useful_jobs(config) # 4
max_useful_jobs(config, from_scratch = TRUE) # 8
max_useful_jobs(config, imports = &quot;files&quot;) # 4
max_useful_jobs(config, imports = &quot;all&quot;) # 8
max_useful_jobs(config, imports = &quot;none&quot;) # 4
</code></pre>

<h2>Caveats</h2>

<p><code>Drake</code> claims that it can</p>

<ol>
<li>Build and cache your targets in parallel (in stages).</li>
<li>Build and cache your targets in the correct order, finishing dependencies before starting targets that depend on them.</li>
<li>Deploy your targets to the parallel backend of your choice.</li>
</ol>

<p>However, the practical efficiency of the parallel computing functionality remains to be verified rigorously. Serious performance studies will be part of future work that has not yet been conducted at the time of writing. In addition, each project has its own best parallel computing set up, and the user needs to optimize it on a case-by-case basis. Some general considerations include the following.</p>

<ul>
<li>The high overhead high scalability of distributed computing versus the low overhead and low scalability of local multicore computing.</li>
<li>The high memory usage of local multicore computing, especially <code>&quot;mclapply&quot;</code> parallelism, as opposed to distributed computing, which can spread the memory demands over the available nodes on a cluster.</li>
<li>The marginal gains of increasing the number of jobs indefinitely, especially in the case of local multicore computing if the number of cores is low.</li>
</ul>

<h1>Parallel backends</h1>

<p><code>Drake</code> has multiple parallel backends, i.e. separate mechanisms for achieving parallelism. Some are low-overhead and limited, others are high-overhead and scalable. Just set the <code>parallelism</code> argument of <code>Make</code> to choose a backend. The best choice usually depends on your project&#39;s intended scale and stage of deployment.</p>

<pre><code class="r">parallelism_choices()
## [1] &quot;mclapply&quot;      &quot;parLapply&quot;     &quot;future&quot;        &quot;future_lapply&quot;
## [5] &quot;Makefile&quot;

parallelism_choices(distributed_only = TRUE)
## [1] &quot;future&quot;        &quot;future_lapply&quot; &quot;Makefile&quot;
</code></pre>

<pre><code class="r">?parallelism_choices  # Read an explanation of each backend.
default_parallelism() # &quot;parLapply&quot; on Windows, &quot;mclapply&quot; everywhere else
## [1] &quot;mclapply&quot;
</code></pre>

<h2>mclapply</h2>

<p>The <code>mclapply</code> backend is powered by the <code>mclapply()</code> function from the <code>parallel</code> package, and it forks multiple processes on your local machine. It spins up quickly, but it lacks scalability, and it does not work on Windows. If you try to call <code>make(.., parallelism = &quot;mclapply&quot;, jobs = 2)</code> on a Windows machine, <code>drake</code> will warn you and then demote <code>jobs</code> to 1.</p>

<h2>parLapply</h2>

<pre><code class="r">make(.., parallelism = &quot;mclapply&quot;, jobs = 2)
</code></pre>

<p>The <code>parLapply</code> backend is powered by the <code>parLapply()</code> function from the <code>parallel</code> package. Like the <code>mclapply</code> backend, <code>parLapply</code> only scales up to a handful of jobs on your local machine. <code>parLapply</code> parallelism works on all platforms, but it takes a few seconds to initialize during each <code>make()</code>. If <code>jobs</code> is less than 2, <code>make()</code> does not bother setting up a parallel socket cluster, opting instead for <code>lapply()</code> to reduce overhead. The default parallel backend is <code>parLapply</code> on Windows machines and <code>mclapply</code> everywhere else. </p>

<pre><code class="r">make(.., parallelism = &quot;parLapply&quot;, jobs = 2)
default_parallelism() # &quot;parLapply&quot; on Windows, &quot;mclapply&quot; everywhere else
</code></pre>

<h2>future_lapply</h2>

<p>The <code>future</code> package unlocks a wide array of powerful parallel backends. The idea is to set up a <code>future::plan()</code> in advance and then call <code>make(parallelism = &quot;future_lapply&quot;)</code>.</p>

<pre><code class="r">library(future)
future::plan()
## sequential:
## - args: function (expr, envir = parent.frame(), substitute = TRUE, lazy = FALSE, seed = NULL, globals = TRUE, local = TRUE, earlySignal = FALSE, label = NULL, ...)
## - tweaked: FALSE
## - call: plan(&quot;default&quot;, .init = FALSE)

future::plan(multicore)
future::plan()
## multicore:
## - args: function (expr, envir = parent.frame(), substitute = TRUE, lazy = FALSE, seed = NULL, globals = TRUE, workers = availableCores(constraints = &quot;multicore&quot;), earlySignal = FALSE, label = NULL, ...)
## - tweaked: FALSE
## - call: future::plan(multicore)
</code></pre>

<p><code>make()</code> knows which <code>future::plan()</code> you selected.</p>

<pre><code class="r">make(my_plan, parallelism = &quot;future_lapply&quot;)
</code></pre>

<p>The <code>multicore</code> plan is the analogue of <code>mclapply</code> parallelism, and the <code>multisession</code> plan is the analogue of <code>parLapply</code> parallelism.</p>

<pre><code class="r">future::plan(multisession(workers = 4)) # Use a max of 4 parallel jobs at a time. # nolint
make(my_plan, parallelism = &quot;future_lapply&quot;)
</code></pre>

<p>You can even deploy to your own parallel socket clusters clusters. You can use <code>future::makeClusterPSOCK()</code> rather than <code>parallel::makePSOCKcluster()</code>.</p>

<pre><code class="r">cl &lt;- future::makeClusterPSOCK(2L, dryrun = TRUE)
future::plan(cluster, workers = cl)
make(my_plan, parallelism = &quot;future_lapply&quot;)
</code></pre>

<p>This approach should allow you to deploy targets to a <a href="https://www.docker.com/what-container">Docker container</a>.</p>

<pre><code class="r">## Setup of Docker worker running rocker and r-base # nolint
## (requires installation of future package)
cl &lt;- future::makeClusterPSOCK(
  &quot;localhost&quot;,
  ## Launch Rscript inside Docker container
  rscript = c(
    &quot;docker&quot;, &quot;run&quot;, &quot;--net=host&quot;, &quot;rocker/r-base&quot;,
    &quot;Rscript&quot;
  ),
  ## Install drake
  rscript_args = c(
    &quot;-e&quot;, shQuote(&quot;install.packages(&#39;drake&#39;)&quot;)
  )
)
future::plan(cluster, workers = cl)
make(my_plan, parallelism = &quot;future_lapply&quot;)
</code></pre>

<p>The <a href="https://github.com/HenrikBengtsson/future.batchtools">future.batchtools</a> package unlocks <a href="https://github.com/HenrikBengtsson/future.batchtools#choosing-batchtools-backend">even more parallel computing functionality</a>, particularly for popular job schedulers such as <a href="https://slurm.schedmd.com/">SLURM</a>,  <a href="http://www.adaptivecomputing.com/products/open-source/torque/">TORQUE</a>, and the <a href="https://supcom.hgc.jp/english/utili_info/manual/uge.html">Univa Grid Engine</a>.</p>

<pre><code class="r">library(future.batchtools)
drake_batchtools_tmpl_file(&quot;slurm&quot;) # Write batchtools.slurm.tmpl.
future::plan(
  batchtools_slurm,
  template = &quot;batchtools.slurm.tmpl&quot;,
  workers = 16
)
make(my_plan, parallelism = &quot;future_lapply&quot;)
</code></pre>

<p>You can even nest parallelism strategies together. In the following example, targets are submitted as jobs on the Univa Grid engine, and then <code>future</code>-style multicore parallelism is applied to each target&#39;s command individually.</p>

<pre><code class="r">drake_batchtools_tmpl_file(&quot;sge&quot;) # Write sge-simple.tmpl.
future::plan(
  list(
    tweak(batchtools_sge, template = &quot;sge-simple.tmpl&quot;),
    multiprocess
  )
)
make(my_plan, parallelism = &quot;future_lapply&quot;)
</code></pre>

<p>For parallelism on clusters and job schedulers, special <a href="https://github.com/mllg/batchtools">batchtools</a> <code>*.tmpl</code> configuration files are required, and the technique is described in the documentation of <a href="https://github.com/mllg/batchtools">batchtools</a>. It is your responsibility to configure these files for your job scheduler. You can find some examples on the <code>inst/templates</code> folders of the <a href="https://github.com/mllg/batchtools/tree/master/inst/templates">batchtools</a> and <a href="https://github.com/HenrikBengtsson/future.batchtools/tree/master/inst/templates">future.batchtools</a> GitHub repositories. <code>Drake</code> has some <a href="https://github.com/ropensci/drake/tree/master/inst/examples">built-in prepackaged example workflows</a> as well. See <code>drake_examples()</code> to view your options, and then <code>drake_example()</code> to write the files for an example.</p>

<pre><code class="r">drake_example(&quot;sge&quot;)   # Sun/Univa Grid Engine workflow and supporting files
drake_example(&quot;slurm&quot;) # SLURM workflow and supporting files
</code></pre>

<p>To just write the <a href="https://github.com/mllg/batchtools">batchtools</a> <code>*.tmpl</code> for an example, use</p>

<pre><code class="r">drake_batchtools_tmpl_file(&quot;sge&quot;)   # Writes sge-simple.tmpl
drake_batchtools_tmpl_file(&quot;slurm&quot;) # Writes batchtools.slurm.tmpl
</code></pre>

<p>Be sure to heed the previously-mentioned cautionary note about deploying too many jobs at once. In <code>&quot;future_lapply&quot;</code> parallelism, the <code>jobs</code> argument applies to the imports, but not the targets. Functions passed to <code>future::plan()</code> such as <code>mulitisession()</code> and <code>batchtools_slurm()</code> usually have a <code>workers</code> arguments for this purpose. Depending on the <code>future</code> backend you select with <code>future::plan()</code>, you might also make use of one of the other environment variables listed in <code>?future::future.options</code>.</p>

<h2>future</h2>

<p>The <code>future</code> backend is experimental and needs more real-world testing. It is similar to <code>future_lapply</code> except that individual futures are launched and managed using a manual job scheduler. Jobs are submitted as soon as workers become available, which overcomes an inefficiency of the usual staged parallelism. And with the optional <code>evaluator</code> column of the workflow plan data frame, you can use different computing resources for different targets. (See the <code>evaluator</code> argument of <code>future()</code>.)</p>

<pre><code class="r">library(future)
library(drake)
load_basic_example()
remote &lt;- future::plan(multisession)
local &lt;- future::plan(multicore)
evaluator &lt;- NULL
# Make the targets with the multisession future backend...
for (i in seq_len(nrow(my_plan))){
  evaluator &lt;- c(evaluator, remote)
}
# ...except for the R Markdown report.
evaluator[[1]] &lt;- local
my_plan$evaluator &lt;- evaluator
make(my_plan, parallelism = &quot;future&quot;, jobs = 8)
</code></pre>

<p>In addition, you can set the <code>caching</code> argument to control when the values of the targets are cached: <code>&quot;worker&quot;</code> for the individual workers (default) and <code>&quot;master&quot;</code> for the master process. If you let the workers do the caching, you can take advantage of parallelism when targets are stored. On the other hand, <code>&quot;master&quot;</code> is a better option if workers do not have cache access or you are using a custom cache that is not thread-safe (e.g. <code>storr::storr_dbi()</code>.</p>

<h2>Makefile</h2>

<p><code>Makefile</code> parallelism uses proper <a href="https://www.gnu.org/software/make/">Makefiles</a> to distribute targets across different R sessions. Similarly to <code>future_lapply</code> parallelism, it is a mechanism for distributing targets at scale. </p>

<h3>Basic Makefile parallelism</h3>

<p>Before running <code>Makefile</code> parallelism, Windows users need to download and install <a href="https://cran.r-project.org/bin/windows/Rtools/"><code>Rtools</code></a>. For everyone else, just make sure <a href="https://www.gnu.org/software/make/">Make</a> is installed. Then, in the next <code>make()</code>, simply set the <code>parallelism</code> and <code>jobs</code> arguments as before.</p>

<pre><code class="r">make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 2)
</code></pre>

<p>You will see a <code>Makefile</code> written to your working directory. Do not run this <code>Makefile</code> separately from <code>drake</code>. It will not work correctly by itself because it depends on the transient dummy timestamp files created by <code>make()</code>. </p>

<p><code>Makefile</code> parallelism has its own modes of flexibility. You can now use the <code>args</code> argument to send custom arguments to the <code>Makefile</code>. For example, you could use 4 parallel jobs for the imports and 6 parallel jobs for the targets.</p>

<pre><code class="r">make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 4, args = &quot;--jobs=6 --silent&quot;)
</code></pre>

<p>The <code>args</code> also let you print out the <code>Makefile</code> without running it, which helps during troubleshooting.</p>

<pre><code class="r">make(my_plan, parallelism = &quot;Makefile&quot;, args = c(&quot;--touch&quot;, &quot;--silent&quot;))
</code></pre>

<p>In addition, you can use a program other than <a href="https://www.gnu.org/software/make/">GNU Make</a>, such as <code>lsmake</code>, to run the <code>Makefile</code>.</p>

<pre><code class="r">make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 4, command = &quot;lsmake&quot;)
</code></pre>

<pre><code class="r">default_Makefile_command()
## [1] &quot;make&quot;
</code></pre>

<p>For finer control over the build process, use the <code>recipe_command</code> argument. By default, the <code>recipe_command</code> is <code>&quot;Rscript -e &#39;R_RECIPE&#39;&quot;</code>.</p>

<pre><code class="r">default_recipe_command()
## [1] &quot;Rscript -e &#39;R_RECIPE&#39;&quot;

r_recipe_wildcard()
## [1] &quot;R_RECIPE&quot;
</code></pre>

<p>The <code>R_RECIPE</code> wildcard is replaced by <code>drake::mk(&quot;your_target&quot;, &quot;path_to_cache&quot;)</code> in the <code>Makefile</code>. That way, a target named <code>your_target</code> is built with the <code>Makefile</code> recipe,</p>

<pre><code>Rscript -e 'drake::mk("your_target", "path_to_cache")'
</code></pre>

<p>You can change the recipe with the <code>recipe_command</code> argument to <code>make()</code>. For example, to save some time and skip the loading of the <code>methods</code> package, you might use <code>&quot;R -e &#39;R_RECIPE&#39; -q&quot;</code>.</p>

<pre><code class="r">make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 4,
  recipe_command = &quot;R -e &#39;R_RECIPE&#39; -q&quot;)
</code></pre>

<p>The <code>Makefile</code> recipe for <code>your_target</code> becomes</p>

<pre><code>R -e 'drake::mk("your_target", "path_to_cache") -q'
</code></pre>

<p>But be warned: that particular recipe fails on Windows.</p>

<p>Use the <code>Makefile_recipe()</code> function to show and tweak <code>Makefile</code> recipes in advance.</p>

<pre><code class="r">Makefile_recipe(cache_path = &quot;just_use_the_default&quot;)
## Rscript -e &#39;drake::mk(target = &quot;your_target&quot;, cache_path = &quot;just_use_the_default&quot;)&#39;

Makefile_recipe(
  recipe_command = &quot;R -e &#39;R_RECIPE&#39; -q&quot;,
  target = &quot;this_target&quot;,
  cache_path = &quot;custom_cache&quot;
)
## R -e &#39;drake::mk(target = &quot;this_target&quot;, cache_path = &quot;custom_cache&quot;)&#39; -q
</code></pre>

<p>If <code>recipe_command</code> contains no mention of <code>R_RECIPE</code>, then <code>R_RECIPE</code> is single-quoted and appended automatically.</p>

<pre><code class="r">Makefile_recipe(recipe_command = &quot;R -q -e&quot;, cache_path = &quot;supplied_by_default&quot;)
## R -q -e &#39;drake::mk(target = &quot;your_target&quot;, cache_path = &quot;supplied_by_default&quot;)&#39;
</code></pre>

<p>Try each of the following and look at the generated <code>Makefile</code> after each call to <code>make()</code>. To see the recipes printed to the console, run <code>clean()</code> between each <code>make()</code> and leave <code>verbose</code> equal to <code>TRUE</code> (default).</p>

<pre><code class="r">make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 4)
make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 4,
  recipe_command = &quot;Rscript -e&quot;)
make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 4,
  recipe_command = &quot;Rscript -e &#39;R_RECIPE&#39;&quot;)
</code></pre>

<p>But do not try the following on Windows.</p>

<pre><code class="r">make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 4,
  recipe_command = &quot;R -e &#39;R_RECIPE&#39; -q&quot;)
make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 4,
  recipe_command = &quot;R -q -e &#39;R_RECIPE&#39;&quot;)
make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 4,
  recipe_command = &quot;R -q -e&quot;)
</code></pre>

<h3>Makefile parallelism on a cluster</h3>

<p>In the general case, you will need a new configuration file to tell the <code>Makefile</code> how to talk to the cluster. The <code>shell_file()</code> function writes a starter.</p>

<pre><code>#!/bin/bash
shift
echo "module load R; $*" | qsub -sync y -cwd -j y
</code></pre>

<p>This file acts as the &ldquo;shell&rdquo; for the <code>Makefile</code> instead of a typical <a href="https://www.gnu.org/software/bash">Unix shell</a>. It is a mechanism for tricking the <code>Makefile</code> into submitting each target as a job on a cluster rather than your local machine. You may need to configure <code>shell.sh</code> for your system, possibly changing <code>module load R</code> to point to the appropriate copy of R.</p>

<p>To tell the <code>Makefile</code> to use <code>shell.sh</code>, you add the line <code>SHELL=./shell.sh</code> to the top of the <code>Makefile</code> using the <code>prepend</code> argument to <code>make()</code>.</p>

<pre><code class="r">make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 2, prepend = &quot;SHELL=./shell.sh&quot;)
</code></pre>

<p><a href="https://slurm.schedmd.com/">SLURM</a> users may be able to <a href="http://plindenbaum.blogspot.com/2014/09/parallelizing-gnu-make-4-in-slurm.html">invoke <code>srun</code> and dispense with <code>shell.sh</code> altogether</a>, though success may vary depending on the SLURM system. You will probably also need to set resource allocation parameters governing memory, runtime, etc. See <code>man srun</code> for the possible <code>.SHELLFLAGS</code>.</p>

<pre><code class="r">make(
  my_plan,
  parallelism = &quot;Makefile&quot;,
  jobs = 2,
  prepend = c(
    &quot;SHELL=srun&quot;,
    &quot;.SHELLFLAGS=-N1 -n1 bash -c&quot;
  )
)
</code></pre>

<p>In some cases, you may be able to use <code>recipe_command</code> to talk to the cluster rather than <code>prepend</code>.</p>

<pre><code class="r">make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 4,
  recipe_command = &quot;tell_cluster_to_submit Rscript -e&quot;)
</code></pre>

<p>Finally, to deploy your work, just save the call to <code>make()</code> in an R script (say, <code>my_script.R</code>) and then launch it from the <a href="https://www.howtogeek.com/140679/beginner-geek-how-to-start-using-the-linux-terminal/">Linux terminal</a>.</p>

<pre><code>nohup nice -19 R CMD BATCH script.R &
</code></pre>

<h1>Drake as an ordinary job scheduler</h1>

<p>If you do not care about reproducibility and you want <code>drake</code> to be an ordinary job scheduler, consider using <a href="https://github.com/ropensci/drake/blob/master/vignettes/debug.Rmd#test-with-triggers">alternative triggers</a>.</p>

<pre><code class="r">load_basic_example()
make(my_plan, trigger = &quot;missing&quot;) # Also consider &quot;always&quot;.
</code></pre>

<p>Above, <code>drake</code> only builds the missing targets. This skips much of the <a href="https://github.com/ropensci/drake/blob/master/vignettes/storage.Rmd#hash-algorithms">time-consuming hashing</a> that ordinarily detects which targets are out of date.</p>

<h1>Final thoughts</h1>

<h2>Debugging</h2>

<p>For large workflows, downsizing and debugging tools become super important. See the <a href="https://github.com/ropensci/drake/blob/master/vignettes/debug.Rmd">&ldquo;debug&rdquo; vignette</a> for help on diagnosing problems with a workflow. <a href="https://github.com/ropensci/drake/blob/master/vignettes/debug.Rmd#test-with-triggers">Triggers</a> and <a href="https://github.com/ropensci/drake/blob/master/vignettes/debug.Rmd#diagnose-failures">cached error logs</a> especially speed the development and testing process.</p>

<h2>Zombies</h2>

<p>In versions of R prior to 3.5.0, some parallel backends, particularly <code>mclapply</code> and <code>future::multicore</code>, may create zombie processes. This issue is fixed in R versions 3.5.0 and later. Zombie children are not usually harmful, but you may wish to kill them yourself. The following function by <a href="https://github.com/CarlBoneri">Carl Boneri</a> should work on Unix-like systems. For a discussion, see <a href="https://github.com/ropensci/drake/issues/116">drake issue 116</a>.</p>

<pre><code class="r">fork_kill_zombies &lt;- function(){
  require(inline)
  includes &lt;- &quot;#include &lt;sys/wait.h&gt;&quot;
  code &lt;- &quot;int wstat; while (waitpid(-1, &amp;wstat, WNOHANG) &gt; 0) {};&quot;

  wait &lt;- inline::cfunction(
    body = code,
    includes = includes,
    convention = &quot;.C&quot;
  )

  invisible(wait())
}
</code></pre>

<h2>More resources</h2>

<p>See the <a href="https://github.com/ropensci/drake/blob/master/vignettes/timing.Rmd">timing vignette</a> for explanations of functions <code>rate_limiting_times()</code> and <code>predict_runtime()</code>, which can help predict the possible speed gains of having multiple independent jobs. If you suspect <code>drake</code> itself is slowing down your project, you may want to read the storage vignette to learn how to set the hashing algorithms of your project.</p>

</body>

</html>
